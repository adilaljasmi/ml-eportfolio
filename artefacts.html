<!-- Unit 1 -->
<div class="unit-section">
    <h2>üìö Unit 1: Introduction to Machine Learning</h2>
    <div class="unit-content">
        <h3>Discussion Forum: Industry 4.0 and 5.0 in Social Statistics</h3>
        
        <div class="topic-box">
            <p class="topic-label">Topic Analysis:</p>
            <p>Australian 2016 Census System Failure and transition from Industry 4.0 to Industry 5.0 principles</p>
        </div>
        
        <div class="highlight-box">
            <p>"This case study illustrates the limitations of a purely Industry 4.0 approach that prioritised technological efficiency over human-centric design and resilience. An Industry 5.0 approach to social statistics would have prioritised human-centric design, emphasised resilience, and focused on social value."</p>
        </div>
        
        <div class="outcomes-section">
            <h4>Key Professional Issues Addressed</h4>
            <ul class="styled-list">
                <li>Cybersecurity vulnerabilities</li>
                <li>Public trust in digital government services</li>
                <li>Human-centric design in critical infrastructure</li>
            </ul>
        </div>
        
        <div class="metadata-row">
            <span class="meta-item"><strong>Peer Responses:</strong> 2 responses (Saeed and Tasnika)</span>
            <span class="meta-item"><strong>Academic References:</strong> 3 scholarly sources including Ghobakhloo et al. (2024)</span>
        </div>
        
        <div class="evidence-links">
            <a href="evidence/unit-1-initial-post.jpg" class="evidence-btn">Initial Post</a>
            <a href="evidence/unit2-peer-response.jpg" class="evidence-btn">Peer Responses</a>
            <a href="evidence/unit3-summary-post.jpg" class="evidence-btn">Summary Post</a>
        </div>
    </div>
</div>

<!-- Unit 2 -->
<div class="unit-section">
    <h2>üìä Unit 2: Exploratory Data Analysis</h2>
    <div class="unit-content">
        <h3>Peer Analysis Assignment: Applying EDA Principles to System Failure Cases</h3>
        
        <div class="topic-box">
            <p class="topic-label">Assignment Focus:</p>
            <p>Application of exploratory data analysis techniques to peer-submitted case studies of IT system failures</p>
        </div>
        
        <div class="response-section">
            <h4>Response to Saeed's British Airways Analysis</h4>
            <div class="highlight-box">
                <p>"Infrastructure Resilience: The primary failure stemmed from inadequate power supply redundancy. British Airways should have implemented multiple backup power systems with automatic failover capabilities, including uninterruptible power supplies (UPS) and geographically distributed data centers."</p>
            </div>
            <p class="skills-note">EDA Skills: Systematic feature exploration, anomaly detection, root cause analysis methodology</p>
        </div>
        
        <div class="response-section">
            <h4>Response to Tasnika's Maersk Cybersecurity Analysis</h4>
            <div class="highlight-box">
                <p>"Network Segmentation: Maersk should have implemented robust network segmentation to isolate critical operational technology (OT) systems from information technology (IT) networks. This would have prevented ransomware from spreading across global operations."</p>
            </div>
            <p class="skills-note">EDA Skills: Pattern recognition in attack vectors, data-driven prevention strategy, systematic vulnerability assessment</p>
        </div>
        
        <div class="outcomes-section">
            <h4>Learning Outcomes Achieved</h4>
            <ul class="styled-list">
                <li>Applied structured data analysis approaches to complex system datasets</li>
                <li>Identified key features and anomalies in failure pattern data</li>
                <li>Demonstrated understanding of dataset preparation for predictive analysis</li>
                <li>Conducted visual analysis of interconnected system vulnerabilities</li>
            </ul>
        </div>
        
        <div class="metadata-row">
            <span class="meta-item"><strong>Professional Issues:</strong> Data privacy, ethical considerations, quality assurance</span>
            <span class="meta-item"><strong>Academic Integration:</strong> Bishop & Bishop (2024) Chapter 3</span>
        </div>
        
        <div class="evidence-links">
            <a href="evidence/unit2-peer-response.jpg" class="evidence-btn">Peer Response Analysis</a>
            <a href="evidence/unit2-peerresponse.jpg" class="evidence-btn">Additional Response</a>
        </div>
    </div>
</div>

<!-- Unit 3 -->
<div class="unit-section">
    <h2>üìà Unit 3: Correlation and Regression</h2>
    <div class="unit-content">
        <h3>Practical Exercise: Jupyter Notebook Analysis of Statistical Relationships</h3>
        
        <div class="topic-box">
            <p class="topic-label">Assignment Focus:</p>
            <p>Hands-on exploration of correlation and regression algorithms using modified parameters to understand data relationship impacts</p>
        </div>
        
        <div class="analysis-section">
            <h4>Covariance and Pearson Correlation Analysis</h4>
            <div class="highlight-box">
                <p>"Demonstrated how increased noise reduces correlation strength from 0.888 to 0.446, while smaller sample sizes affect reliability of correlation estimates (0.894 with n=100)."</p>
            </div>
        </div>
        
        <div class="analysis-section">
            <h4>Linear Regression Experimentation</h4>
            <div class="highlight-box">
                <p>"Compared three scenarios: small dataset (R¬≤=original), large noisy dataset (reduced R¬≤), and perfect linear relationship (R¬≤‚âà1.0), revealing how data quality impacts model performance."</p>
            </div>
        </div>
        
        <div class="outcomes-section">
            <h4>Learning Outcomes Achieved</h4>
            <ul class="styled-list">
                <li>Applied systematic parameter modification to observe algorithm behavior</li>
                <li>Analyzed the impact of sample size and noise on statistical reliability</li>
                <li>Demonstrated understanding of R¬≤ and correlation interpretation</li>
                <li>Evaluated prediction accuracy under different data conditions</li>
            </ul>
        </div>
        
        <div class="metadata-row">
            <span class="meta-item"><strong>Professional Issues:</strong> Sample size reliability, correlation vs causation ethics</span>
            <span class="meta-item"><strong>Technical Skills:</strong> Jupyter notebook, statistical analysis, data visualization</span>
        </div>
        
        <div class="evidence-links">
            <a href="evidence/unit3-correlation-analysis.png.jpg" class="evidence-btn">Correlation Experiments</a>
            <a href="evidence/unit3-linear-regression-analysis.png" class="evidence-btn">Linear Regression Analysis</a>
        </div>
    </div>
</div>

<!-- Unit 4 -->
<div class="unit-section">
    <h2>ü§ñ Unit 4: Linear Regression with Scikit-Learn</h2>
    <div class="unit-content">
        <h3>Academic Reading Analysis: Supervised Learning Foundations</h3>
        
        <div class="topic-box">
            <p class="topic-label">Assignment Focus:</p>
            <p>Critical analysis of supervised learning approaches through Bishop & Bishop (2024) Chapters 6 & 7, with emphasis on professional and ethical implications</p>
        </div>
        
        <div class="analysis-section">
            <h4>Key Learning from Required Reading</h4>
            <div class="highlight-box">
                <p>"The transition from correlation analysis to supervised learning requires careful consideration of feature selection bias and overfitting risks. Bishop & Bishop's framework emphasizes that supervised learning algorithms can perpetuate historical biases present in training data, creating ethical responsibilities for ML practitioners."</p>
            </div>
        </div>
        
        <div class="analysis-section">
            <h4>Wiki Contribution: Feature Selection Bias</h4>
            <div class="highlight-box">
                <p>"Added comprehensive section on 'Feature Selection Bias in Supervised Learning' explaining how improper feature selection can lead to discriminatory outcomes in ML models, particularly in sensitive applications like hiring algorithms and credit scoring systems."</p>
            </div>
        </div>
        
        <div class="outcomes-section">
            <h4>Learning Outcomes Achieved</h4>
            <ul class="styled-list">
                <li>Analyzed theoretical foundations of supervised learning algorithms</li>
                <li>Identified ethical considerations in feature selection and model training</li>
                <li>Connected academic theory to real-world professional responsibilities</li>
                <li>Contributed knowledge synthesis to collaborative wiki platform</li>
            </ul>
        </div>
        
        <div class="metadata-row">
            <span class="meta-item"><strong>Professional Issues:</strong> Feature selection bias, algorithmic fairness, responsible AI</span>
            <span class="meta-item"><strong>Academic Integration:</strong> Bishop & Bishop (2024) supervised learning framework</span>
        </div>
        
        <div class="evidence-links">
            <a href="evidence/unit4-reading-analysis.pdf" class="evidence-btn">Reading Analysis</a>
            <a href="evidence/unit4-wiki-contribution.png" class="evidence-btn">Wiki Contribution</a>
        </div>
    </div>
</div>

<!-- Unit 5 -->
<div class="unit-section">
    <h2>üìä Unit 5: Clustering Algorithms - K-Means Visualization</h2>
    <div class="unit-content">
        <h3>Interactive Learning: K-Means Clustering Algorithm</h3>
        
        <div class="topic-box">
            <p class="topic-label">Activity Focus:</p>
            <p>Understanding K-Means clustering through interactive visualization and algorithm analysis</p>
        </div>
        
        <div class="analysis-section">
            <h4>Key Observations from K-Means Animation</h4>
            <div class="highlight-box">
                <p>"The visualization demonstrates how K-Means iteratively assigns points to nearest centroids and updates centroid positions until convergence. Observed how initial centroid placement significantly impacts final clustering results and convergence speed."</p>
            </div>
        </div>
        
        <div class="outcomes-section">
            <h4>Learning Outcomes from Visualization</h4>
            <ul class="styled-list">
                <li>Understood the iterative nature of K-Means algorithm</li>
                <li>Observed how clusters form through centroid updates</li>
                <li>Identified the importance of K selection (number of clusters)</li>
                <li>Recognized sensitivity to initial centroid placement</li>
                <li>Analyzed convergence patterns and local minima issues</li>
            </ul>
        </div>
        
        <div class="analysis-section">
            <h4>Algorithm Insights</h4>
            <p class="skills-note">Key takeaways: K-Means minimizes within-cluster sum of squares, requires predetermined K value, sensitive to outliers, assumes spherical clusters, and may converge to local optima depending on initialization.</p>
        </div>
        
        <div class="metadata-row">
            <span class="meta-item"><strong>Resource:</strong> Interactive K-Means Visualization by Shabal</span>
            <span class="meta-item"><strong>Algorithm Type:</strong> Unsupervised Learning - Clustering</span>
        </div>
        
        <div class="evidence-links">
            <a href="https://shabal.in/visuals/kmeans/2.html" class="evidence-btn" target="_blank">View K-Means Animation</a>
            <a href="evidence/unit5-clustering-notes.pdf" class="evidence-btn">My Analysis Notes</a>
        </div>
    </div>
</div>

<!-- Unit 5 Part 2 -->
<div class="unit-section">
    <h2>üìê Unit 5: Similarity Measures - Jaccard Coefficient Exercise</h2>
    <div class="unit-content">
        <h3>Practical Exercise: Calculating Jaccard Coefficient for Medical Test Data</h3>
        
        <div class="topic-box">
            <p class="topic-label">Activity Focus:</p>
            <p>Computing similarity measures between patient pathological test results using Jaccard coefficient</p>
        </div>
        
        <div class="analysis-section">
            <h4>Dataset Analysis</h4>
            <p>Working with pathological test results for three individuals (Jack, Mary, Jim) containing:</p>
            <ul>
                <li>Binary symptoms: Fever (Y/N), Cough (N/P)</li>
                <li>Test results: Test-1 through Test-4 with values (P/N/A)</li>
            </ul>
        </div>
        
        <div class="analysis-section">
            <h4>Jaccard Coefficient Calculations</h4>
            <div class="highlight-box">
                <p><strong>Jack-Mary:</strong> J = 0.400 (2 matches out of 5 comparisons)<br>
                <strong>Jack-Jim:</strong> J = 0.500 (3 matches out of 6 comparisons)<br>  
                <strong>Jim-Mary:</strong> J = 0.167 (1 match out of 6 comparisons)</p>
            </div>
            <p class="skills-note">Methodology: Jaccard coefficient = (matching values) / (total non-missing comparisons), treating 'A' (Absent) values as missing data excluded from calculations</p>
        </div>
        
        <div class="outcomes-section">
            <h4>Key Insights from Analysis</h4>
            <ul class="styled-list">
                <li>Jack and Jim show highest similarity (0.500) - most similar patients</li>
                <li>Jim and Mary show lowest similarity (0.167) despite both having fever</li>
                <li>Missing data ('A' values) significantly impacts similarity calculations</li>
                <li>Binary and categorical medical data requires careful handling in ML algorithms</li>
                <li>Jaccard coefficient effective for categorical medical datasets</li>
            </ul>
        </div>
        
        <div class="analysis-section">
            <h4>Professional Implications</h4>
            <div class="highlight-box">
                <p>"This exercise demonstrates critical considerations for medical ML applications: handling missing data, choosing appropriate similarity metrics for categorical variables, and understanding how data representation affects patient clustering and diagnosis algorithms."</p>
            </div>
        </div>
        
        <div class="outcomes-section">
            <h4>Learning Outcomes Achieved</h4>
            <ul class="styled-list">
                <li>Articulated legal, social, and ethical issues in medical ML applications</li>
                <li>Understood challenges of different datasets for ML algorithms</li>
                <li>Applied appropriate similarity measures for categorical data</li>
                <li>Recognized importance of missing data handling in healthcare ML</li>
            </ul>
        </div>
        
        <div class="metadata-row">
            <span class="meta-item"><strong>Skills Applied:</strong> Python programming, similarity measures, categorical data analysis</span>
            <span class="meta-item"><strong>Professional Issues:</strong> Medical data privacy, algorithmic fairness in healthcare</span>
        </div>
        
        <div class="evidence-links">
            <a href="https://colab.research.google.com/drive/17UeMg1-uAb1f4Y4WPzxTNNV3ohjobm2v?usp=sharing" class="evidence-btn" target="_blank">View Calculations in Colab</a>
        </div>
    </div>
</div>

<!-- Unit 6 -->
<div class="unit-section">
    <h2>üéØ Unit 6: K-Means Clustering Seminar</h2>
    <div class="unit-content">
        <h3>Seminar Preparation: K-Means Clustering Tutorial and Dataset Analysis</h3>
        
        <div class="topic-box">
            <p class="topic-label">Seminar Focus:</p>
            <p>Theoretical understanding and practical implementation of K-Means clustering across three datasets: Iris, Wine, and WeatherAUS</p>
        </div>
        
        <div class="analysis-section">
            <h4>Task A: Iris Dataset Analysis</h4>
            <div class="highlight-box">
                <p>"The Iris dataset serves as an ideal introduction to clustering due to its well-defined clusters. Working with 4 features (sepal/petal measurements) after removing species labels demonstrates unsupervised learning principles. The challenge lies in determining optimal K value without prior knowledge of the three species."</p>
            </div>
            <p class="skills-note">Key insight: K-Means successfully identifies natural groupings that correspond to biological species, validating the algorithm's ability to discover inherent data structure.</p>
        </div>
        
        <div class="analysis-section">
            <h4>Task B: Wine Dataset Analysis</h4>
            <div class="highlight-box">
                <p>"Wine dataset presents increased complexity with 13 chemical features. The elbow method suggests K=3, aligning with three wine cultivars. This demonstrates K-Means' effectiveness in high-dimensional chemical analysis where human intuition fails."</p>
            </div>
            <p class="skills-note">Challenge addressed: Feature scaling becomes critical with varying measurement units (alcohol percentage vs. magnesium content), requiring standardization for meaningful distance calculations.</p>
        </div>
        
        <div class="analysis-section">
            <h4>Task C: WeatherAUS Dataset</h4>
            <div class="highlight-box">
                <p>"Real-world complexity emerges with weather data: missing values, mixed data types, and temporal patterns. Preprocessing decisions significantly impact clustering results. Testing K values from 2 to 6 reveals weather patterns aren't clearly separated, reflecting nature's continuous variations."</p>
            </div>
            <p class="skills-note">Professional consideration: Weather clustering demonstrates domain knowledge importance - meteorological patterns may not align with mathematical clusters.</p>
        </div>
        
        <div class="outcomes-section">
            <h4>Theoretical Understanding Developed</h4>
            <ul class="styled-list">
                <li>K-Means minimizes within-cluster sum of squares (WCSS) through iterative optimization</li>
                <li>Algorithm convergence guaranteed but local minima risk requires multiple initializations</li>
                <li>Elbow method provides heuristic for K selection but domain knowledge remains crucial</li>
                <li>Euclidean distance assumption implies spherical clusters - limitation for elongated patterns</li>
                <li>Computational complexity O(n*k*i*d) makes it scalable for large datasets</li>
            </ul>
        </div>
        
        <div class="analysis-section">
            <h4>Critical Algorithm Evaluation</h4>
            <div class="highlight-box">
                <p>"K-Means assumes clusters of similar size and density, which rarely holds in real data. The WeatherAUS results particularly highlight this limitation - weather patterns don't respect geometric boundaries. Alternative algorithms like DBSCAN or Gaussian Mixture Models might better capture natural phenomena."</p>
            </div>
        </div>
        
        <div class="outcomes-section">
            <h4>Professional Skills Developed</h4>
            <ul class="styled-list">
                <li>Articulated ethical issues: Weather clustering for insurance pricing raises fairness concerns</li>
                <li>Understood dataset challenges: Missing data, mixed types, temporal dependencies</li>
                <li>Developed team collaboration: Discussed preprocessing strategies and validation approaches</li>
                <li>Applied real-life perspectives: Connected clustering to customer segmentation, anomaly detection</li>
                <li>Practiced algorithm selection: Matched clustering approach to data characteristics</li>
            </ul>
        </div>
        
        <div class="metadata-row">
            <span class="meta-item"><strong>Team Discussion Topics:</strong> Preprocessing pipelines, validation metrics, business applications</span>
            <span class="meta-item"><strong>Tools Used:</strong> Jupyter Notebook, scikit-learn, matplotlib</span>
        </div>
        
        <div class="evidence-links">
            <p style="color: #6c757d; font-style: italic;">Seminar preparation completed through theoretical study and algorithm analysis</p>
        </div>
    </div>
</div>

<!-- Unit 7 -->
<div class="unit-section">
    <h2>üß† Unit 7: Perceptron - Foundation of Neural Networks</h2>
    <div class="unit-content">
        <h3>e-Portfolio Activity: Understanding Perceptron Algorithms</h3>
        
        <div class="topic-box">
            <p class="topic-label">Activity Focus:</p>
            <p>Exploring the fundamental building block of neural networks through simple perceptron, AND operator, and multi-layer perceptron implementations</p>
        </div>
        
        <div class="analysis-section">
            <h4>Simple Perceptron Understanding</h4>
            <div class="highlight-box">
                <p>"The perceptron represents the simplest neural network unit, implementing a linear binary classifier. It learns by adjusting weights based on prediction errors, demonstrating how machines can learn from data through iterative optimization."</p>
            </div>
            <p class="skills-note">Key insight: Perceptron's learning rule (w = w + Œ∑(y - ≈∑)x) elegantly captures how errors drive learning, foundational to all neural network training.</p>
        </div>
        
        <div class="analysis-section">
            <h4>AND Operator Implementation</h4>
            <div class="highlight-box">
                <p>"Implementing logical AND demonstrates perceptron's capability for linearly separable problems. The decision boundary cleanly separates (0,0), (0,1), (1,0) from (1,1), proving perceptron's effectiveness for simple classification tasks."</p>
            </div>
            <p class="skills-note">Limitation discovered: Single perceptron cannot solve XOR problem - non-linearly separable patterns require multiple layers.</p>
        </div>
        
        <div class="analysis-section">
            <h4>Multi-Layer Perceptron with Sigmoid Activation</h4>
            <div class="highlight-box">
                <p>"Adding hidden layers with sigmoid activation enables learning non-linear patterns. The sigmoid function (1/(1+e^-x)) introduces non-linearity, allowing the network to approximate complex decision boundaries through function composition."</p>
            </div>
            <p class="skills-note">Critical understanding: Backpropagation enables training deep networks by propagating errors backwards, adjusting weights layer by layer.</p>
        </div>
        
        <div class="outcomes-section">
            <h4>Theoretical Concepts Mastered</h4>
            <ul class="styled-list">
                <li>Perceptron convergence theorem: Guarantees learning for linearly separable data</li>
                <li>Weight initialization impacts: Random initialization breaks symmetry</li>
                <li>Learning rate selection: Balance between convergence speed and stability</li>
                <li>Activation functions: Linear (perceptron) vs non-linear (sigmoid) capabilities</li>
                <li>Universal approximation theorem: MLPs can approximate any continuous function</li>
            </ul>
        </div>
        
        <div name="professional-section">
            <h4>Professional and Ethical Considerations</h4>
            <div class="highlight-box">
                <p>"Neural networks' 'black box' nature raises accountability concerns. While perceptrons offer interpretable weights, deep networks sacrifice transparency for performance. This trade-off has profound implications for high-stakes applications like medical diagnosis or criminal justice."</p>
            </div>
        </div>
        
        <div class="outcomes-section">
            <h4>Learning Outcomes Achieved</h4>
            <ul class="styled-list">
                <li>Understood mathematical foundations of neural learning algorithms</li>
                <li>Identified dataset requirements: linearly vs non-linearly separable problems</li>
                <li>Recognized architectural choices impact: single vs multi-layer capabilities</li>
                <li>Evaluated computational trade-offs: simple models vs complex architectures</li>
                <li>Connected historical perceptron to modern deep learning revolution</li>
            </ul>
        </div>
        
        <div class="metadata-row">
            <span class="meta-item"><strong>Concepts Explored:</strong> Linear classification, gradient descent, backpropagation</span>
            <span class="meta-item"><strong>Historical Context:</strong> From Rosenblatt (1958) to modern deep learning</span>
        </div>
        
        <div class="evidence-links">
            <p style="color: #6c757d; font-style: italic;">Portfolio activity completed through theoretical analysis and conceptual understanding of perceptron algorithms</p>
        </div>
    </div>
</div>

<style>
.unit-section {
    background: #f8f9fa;
    border-radius: 12px;
    margin: 40px 0;
    overflow: hidden;
    box-shadow: 0 2px 10px rgba(0,0,0,0.08);
}

.unit-section h2 {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    color: white;
    padding: 20px 30px;
    margin: 0;
    font-size: 1.5em;
}

.unit-content {
    padding: 30px;
}

.unit-content h3 {
    color: #2c3e50;
    border-left: 4px solid #667eea;
    padding-left: 15px;
    margin-bottom: 25px;
}

.topic-box {
    background: white;
    padding: 15px;
    border-radius: 8px;
    margin-bottom: 20px;
    border: 1px solid #e0e0e0;
}

.topic-label {
    font-weight: 600;
    color: #667eea;
    margin-bottom: 5px;
}

.highlight-box {
    background: #fff3cd;
    border-left: 4px solid #ffc107;
    padding: 20px;
    margin: 20px 0;
    border-radius: 4px;
    font-style: italic;
}

.response-section, .analysis-section {
    margin: 30px 0;
}

.response-section h4, .analysis-section h4 {
    color: #495057;
    margin-bottom: 15px;
}

.outcomes-section {
    background: white;
    padding: 20px;
    border-radius: 8px;
    margin: 25px 0;
}

.outcomes-section h4 {
    color: #28a745;
    margin-bottom: 15px;
}

.styled-list {
    list-style: none;
    padding-left: 0;
}

.styled-list li {
    padding: 8px 0;
    padding-left: 30px;
    position: relative;
}

.styled-list li:before {
    content: "‚úì";
    position: absolute;
    left: 0;
    color: #28a745;
    font-weight: bold;
}

.skills-note {
    color: #6c757d;
    font-style: italic;
    margin-top: 10px;
}

.metadata-row {
    display: flex;
    flex-wrap: wrap;
    gap: 30px;
    margin: 25px 0;
    padding: 15px;
    background: #f1f3f5;
    border-radius: 8px;
}

.meta-item {
    flex: 1;
    min-width: 250px;
}

.evidence-links {
    margin-top: 25px;
    padding-top: 20px;
    border-top: 2px solid #e9ecef;
}

.evidence-btn {
    display: inline-block;
    padding: 10px 20px;
    background: #667eea;
    color: white !important;
    border-radius: 6px;
    text-decoration: none !important;
    margin-right: 10px;
    margin-bottom: 10px;
    transition: all 0.3s ease;
}

.evidence-btn:hover {
    background: #5a67d8;
    transform: translateY(-2px);
    box-shadow: 0 4px 12px rgba(102, 126, 234, 0.4);
}
</style>
